Study Dataset & Analysis — README

This repository contains study data from 20 participants and scripts to reproduce the inter-rater/monitoring agreement analysis and generate the forest plot used as Figure 3 in the paper.

⸻

Repository structure

.
├── participant_study_data/
│   ├── P1/
│   │   ├── instructions/
│   │   │   ├── exercise1.json
│   │   │   └── exercise2.json
│   │   ├── results/
│   │   │   ├── exercise1_results.json
│   │   │   └── exercise2_results.json
│   │   ├── scenic_programs/
│   │   │   ├── exercise1.scenic
│   │   │   └── exercise2.scenic
│   │   └── summaries/
│   │       ├── exercise1_summary.log
│   │       └── exercise2_summary.log
│   ├── P2/
│   │   └── ...
│   └── P20/
│       └── ...
├── Exercises/
│   ├── template.txt
│   ├── ex1.txt
│   ├── ...
│   └── ex10.txt
├── requirement.txt
├── data_load.py
├── forest_plot.R
└── README.md

participant_study_data/
	•	P1 … P20 — per-participant subdirectories.
	1.	instructions/ — two JSON files with the instructions for the two exercises designed by the participant.
	2.	results/ — two JSON files comparing the participant’s binary labels with the LLM-generated software’s monitored binary labels.
	•	Binary labels: True = instruction completed; False = incompletion.
	•	Each JSON includes:
	•	stepsAccuracy: percentage of instructions judged by the participant to be adequately paced.
	•	successfulFailedOmittedAccuracy: percentage of the software’s monitored labels that match the participant’s labels.
	•	therapistReport: dictionary with the participant’s prescribed instructions (exerciseStep) and their labels (successfulInstructions).
	•	llmReport: dictionary with the same prescribed instructions (exerciseStep) and the software’s monitored labels (successfulInstructions).
Example schema (comments for clarity):

{
  "stepsAccuracy": <number>,                       // percentage (0–100)
  "successfulFailedOmittedAccuracy": <number>,     // percentage (0–100)
  "therapistReport": {
    "exerciseStep": [ "<step 1>", "<step 2>", "..."],
    "successfulInstructions": [ true, false, ... ]  // participant labels
  },
  "llmReport": {
    "exerciseStep": [ "<step 1>", "<step 2>", "..."],
    "successfulInstructions": [ true, false, ... ]  // software-monitored labels
  }
}


	3.	scenic_programs/ — two Scenic programs (i.e., intervention software) generated by the LLM.
	4.	summaries/ — logs of monitored data collected by the two Scenic programs.

Exercises/
	•	template.txt — worksheet instructions prepared for each of the 10 exercise goals.
	•	ex1.txt … ex10.txt — for each exercise goal, instructions from four therapist-designed exercises (one per therapist).
Example: ex1.txt contains the four exercises designed for exercise goal #1.

⸻

How to run the analysis

1) Create a Python 3.11 virtual environment

macOS/Linux:

python3.11 -m venv .venv
source .venv/bin/activate
python -V   # should show Python 3.11.x

Windows (PowerShell):

py -3.11 -m venv .venv
.venv\Scripts\Activate.ps1
python -V   # should show Python 3.11.x

2) Install Python dependencies

Dependencies are listed in requirement.txt.

pip install --upgrade pip
pip install -r requirement.txt

3) Generate the CSV used for ICC analysis

This will read the repository data and write data_for_ICC.csv to the project root.

python data_load.py

	•	The structure and meaning of columns in data_for_ICC.csv are annotated inside the ICC_data_processing() function in data_load.py.

4) Install R

Install R (version 4.5.1 was used for the analysis). Newer versions may also work.
	•	Verify:

R --version



5) Create the forest plot (Figure 3)

Run the R script to produce the forest plot based on data_for_ICC.csv.

Rscript --vanilla forest_plot.R data_for_ICC.csv

	•	The script generates the forest plot used as Figure 3 in the paper. See forest_plot.R for the exact output filename and format.

⸻

Reproducibility notes
	•	Ensure the virtual environment is active when running Python scripts.
	•	If Rscript is not found, confirm that R is installed and that the R binaries are on your system PATH.

⸻

End of README.